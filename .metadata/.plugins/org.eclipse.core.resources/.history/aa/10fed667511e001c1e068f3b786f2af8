package com.finalbdt.spark;

import java.io.IOException;
import java.util.*;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
//import org.apache.spark.streaming.kafka.KafkaUtils;

import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;

import com.finalbdt.kafka.KafkaConfig;
import com.google.gson.Gson;
import com.google.gson.JsonObject;

import org.apache.kafka.clients.consumer.ConsumerRecord;

import kafka.serializer.StringDecoder;
import scala.Tuple2;

public class SparkListener {
	static Map<String, Object> kafkaParams = new HashMap<String, Object>() {{
		put("bootstrap.servers", "127.0.0.1:9092");
	}};
	
	static Set<String> topics = Collections.singleton(KafkaConfig.TOPIC);
	
//	static HBaseTweets hbaseTweets;
//	static{
//		try {
//			hbaseTweets= HBaseTweets.getInstance();
//		} catch (IOException e) {
//			// TODO Auto-generated catch block
//			e.printStackTrace();
//		}
//		}
	
	public static void main(String[] args) throws InterruptedException, IOException {
		HBaseTweets hbaseTweets = HBaseTweets.getInstance();
		SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("FinalBDT");
		JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(10));
		JavaInputDStream<ConsumerRecord<String, String>> stream =

                KafkaUtils.createDirectStream(

                  jssc,
                  LocationStrategies.PreferConsistent(),

                  ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams)

                );
		
		JavaDStream<String> jdd = stream.map(s -> s.value());
		jdd.foreachRDD((JavaRDD<String> rdd) -> {
			rdd.map(s -> new Gson().fromJson(s, JsonObject.class))
			.filter(s -> s.has("delete"))
			.map(Tweet::parseJson).foreach(tweet -> {
				System.out.println(tweet);
				hbaseTweets.addTweet(tweet);
			});
		});

//		JavaInputDStream<String,String> stream = KafkaUtils.createDirectStream(jssc,
//						String.class,
//						String.class,
//						StringDecoder.class,
//						StringDecoder.class, kafkaParams, topics);

//		stream.foreachRDD((JavaPairRDD<String, String> rdd) -> {
//			rdd.values()
//					.map(s -> new Gson().fromJson(s, JsonObject.class))
//					.filter(s -> s.has("delete"))
//					.map(Tweet::parseJson).foreach(tweet -> {
//						System.out.println(tweet);
//						hbaseTweets.addTweet(tweet);
//					});
//		});
		
		jssc.start();
		jssc.awaitTermination();
	}
}
