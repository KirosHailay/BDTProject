package com.finalbdt.spark;

import java.io.IOException;
import java.util.*;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.*;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;

import com.finalbdt.kafka.KafkaConfig;
import com.google.gson.Gson;
import com.google.gson.JsonObject;

import org.apache.kafka.clients.consumer.ConsumerRecord;

import kafka.serializer.StringDecoder;
import scala.Tuple2;

public class SparkListener {
	static Map<String, String> kafkaParams = new HashMap<String, String>() {{
		put("bootstrap.servers", "127.0.0.1:9092");
	}};
	
	static Set<String> topics = Collections.singleton(KafkaConfig.TOPIC);
	
//	static HBaseTweets hbaseTweets;
//	static{
//		try {
//			hbaseTweets= HBaseTweets.getInstance();
//		} catch (IOException e) {
//			// TODO Auto-generated catch block
//			e.printStackTrace();
//		}
//		}
	
	public static void main(String[] args) throws InterruptedException, IOException {
		HBaseTweets hbaseTweets = HBaseTweets.getInstance();
		SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("FinalBDT");
		JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(10));
		JavaPairInputDStream<String,String> stream = KafkaUtils.createDirectStream(jssc,
						String.class,
						String.class,
						StringDecoder.class,
						StringDecoder.class, kafkaParams, topics);

		stream.foreachRDD((JavaPairRDD<String, String> rdd) -> {
			rdd.values()
					.map(s -> new Gson().fromJson(s, JsonObject.class))
					.filter(s -> s.has("delete"))
					.map(Tweet::parseJson).foreach(tweet -> {
						System.out.println(tweet);
						hbaseTweets.addTweet(tweet);
					});
		});
		
		jssc.start();
		jssc.awaitTermination();
	}
}
